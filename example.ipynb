{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/user/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# role = get_execution_role()\n",
    "# print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/user/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "pytorch_model = PyTorchModel(\n",
    "    name='embeddings-model',\n",
    "    model_data='s3://llama-weights-us/labse-model/model.tar.gz',\n",
    "    role=os.environ.get('SAGEMAKER_ROLE'),\n",
    "    framework_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"inference.py\",\n",
    "    env={\"MODEL_NAME\": \"all-MiniLM-L12-v2\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/user/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: embeddings-model\n"
     ]
    }
   ],
   "source": [
    "transformer = pytorch_model.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    accept='text/csv'\n",
    "    )\n",
    "\n",
    "\n",
    "# we can run it directly\n",
    "# transformer.transform(\n",
    "#     \"s3://llama-weights-us/input_data/\",\n",
    "#     content_type=\"text/csv\",\n",
    "#     split_type=\"Line\",\n",
    "#     wait=True,\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/user/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "transformer = Transformer(\n",
    "        model_name='embeddings-model',\n",
    "        output_path=\"s3://llama-weights-us/input_data/\",\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.g4dn.xlarge\",\n",
    "        accept='text/csv',\n",
    "        assemble_with='Line',\n",
    "        strategy='MultiRecord',\n",
    "        max_payload=70,\n",
    "        env={\n",
    "            'BATCH_SIZE': str(64),\n",
    "            'STRATEGY': 'MultiRecord',\n",
    "            'MODEL_NAME': \"all-MiniLM-L12-v2\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: pytorch-inference-2023-11-24-16-15-47-877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................Collecting datasets==2.15.0 (from -r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 521.2/521.2 kB 25.2 MB/s eta 0:00:00\n",
      "Collecting sentence-transformers==2.2.2 (from -r /opt/ml/model/code/requirements.txt (line 2))\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.0/86.0 kB 46.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.22.4)\n",
      "Collecting pyarrow>=8.0.0 (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.0/38.0 MB 49.9 MB/s eta 0:00:00\n",
      "Collecting pyarrow-hotfix (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 58.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (4.64.1)\n",
      "Collecting xxhash (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 55.3 MB/s eta 0:00:00\n",
      "Collecting multiprocess (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 14.9 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.4/166.4 kB 53.5 MB/s eta 0:00:00\n",
      "Collecting aiohttp (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading aiohttp-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 84.7 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.18.0 (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 311.7/311.7 kB 73.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (5.4.1)\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2))\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 122.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2)) (2.0.0+cu118)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2)) (0.15.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2)) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2)) (1.10.1)\n",
      "Collecting nltk (from sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2))\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 102.4 MB/s eta 0:00:00\n",
      "Collecting sentencepiece (from sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2))\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 94.0 MB/s eta 0:00:00\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 30.1 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 8.0 MB/s eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading yarl-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 300.7/300.7 kB 72.9 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.7/225.7 kB 85.2 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (4.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2023.5.7)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2)) (3.1.2)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2))\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 773.9/773.9 kB 139.6 MB/s eta 0:00:00\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2))\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 150.4 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2))\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 145.0 MB/s eta 0:00:00\n",
      "Collecting click (from nltk->sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2))\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 54.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2)) (9.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.15.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2->-r /opt/ml/model/code/requirements.txt (line 2)) (1.3.0)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=1597ce38e9f8fe12ab9d0fb87ca9c782de009bc877444ebd9be937b1fa581279\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentencepiece, xxhash, safetensors, regex, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, click, attrs, async-timeout, yarl, nltk, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, sentence-transformers, datasets\n",
      "Successfully installed aiohttp-3.9.0 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 click-8.1.7 datasets-2.15.0 dill-0.3.7 frozenlist-1.4.0 fsspec-2023.10.0 huggingface-hub-0.19.4 multidict-6.0.4 multiprocess-0.70.15 nltk-3.8.1 pyarrow-14.0.1 pyarrow-hotfix-0.6 regex-2023.10.3 safetensors-0.4.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.15.0 transformers-4.35.2 xxhash-3.4.1 yarl-1.9.3\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "Warning: TorchServe is using non-default JVM parameters: -XX:-UseContainerSupport\n",
      "Successfully installed aiohttp-3.9.0 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 click-8.1.7 datasets-2.15.0 dill-0.3.7 frozenlist-1.4.0 fsspec-2023.10.0 huggingface-hub-0.19.4 multidict-6.0.4 multiprocess-0.70.15 nltk-3.8.1 pyarrow-14.0.1 pyarrow-hotfix-0.6 regex-2023.10.3 safetensors-0.4.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.15.0 transformers-4.35.2 xxhash-3.4.1 yarl-1.9.3\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "Warning: TorchServe is using non-default JVM parameters: -XX:-UseContainerSupport\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-11-24T16:24:42,980 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
      "2023-11-24T16:24:43,041 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml\n",
      "2023-11-24T16:24:43,136 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "Torchserve version: 0.8.0\n",
      "TS Home: /opt/conda/lib/python3.10/site-packages\n",
      "Current directory: /\n",
      "Temp directory: /home/model-server/tmp\n",
      "Metrics config path: /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml\n",
      "Number of GPUs: 1\n",
      "Number of CPUs: 4\n",
      "Max heap size: 3934 M\n",
      "Python executable: /opt/conda/bin/python3.10\n",
      "Config file: /etc/sagemaker-ts.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8080\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-11-24T16:24:42,980 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
      "2023-11-24T16:24:43,041 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml\n",
      "2023-11-24T16:24:43,136 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "Torchserve version: 0.8.0\n",
      "TS Home: /opt/conda/lib/python3.10/site-packages\n",
      "Current directory: /\n",
      "Temp directory: /home/model-server/tmp\n",
      "Metrics config path: /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml\n",
      "Number of GPUs: 1\n",
      "Number of CPUs: 4\n",
      "Max heap size: 3934 M\n",
      "Python executable: /opt/conda/bin/python3.10\n",
      "Config file: /etc/sagemaker-ts.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8080\n",
      "Metrics address: http://127.0.0.1:8082\n",
      "Model Store: /.sagemaker/ts/models\n",
      "Initial Models: model=/opt/ml/model\n",
      "Log dir: /logs\n",
      "Metrics dir: /logs\n",
      "Netty threads: 0\n",
      "Netty client threads: 0\n",
      "Default workers per model: 1\n",
      "Blacklist Regex: N/A\n",
      "Maximum Response Size: 6553500\n",
      "Maximum Request Size: 6553500\n",
      "Limit Maximum Image Pixels: true\n",
      "Prefer direct buffer: false\n",
      "Allowed Urls: [file://.*|http(s)?://.*]\n",
      "Custom python dependency for model allowed: false\n",
      "Enable metrics API: true\n",
      "Metrics mode: log\n",
      "Disable system metrics: true\n",
      "Workflow Store: /.sagemaker/ts/models\n",
      "Model config: N/A\n",
      "2023-11-24T16:24:43,143 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
      "2023-11-24T16:24:43,162 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\n",
      "2023-11-24T16:24:43,166 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "2023-11-24T16:24:43,166 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "2023-11-24T16:24:43,169 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "2023-11-24T16:24:43,179 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "2023-11-24T16:24:43,262 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "2023-11-24T16:24:43,263 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "Metrics address: http://127.0.0.1:8082\n",
      "Model Store: /.sagemaker/ts/models\n",
      "Initial Models: model=/opt/ml/model\n",
      "Log dir: /logs\n",
      "Metrics dir: /logs\n",
      "Netty threads: 0\n",
      "Netty client threads: 0\n",
      "Default workers per model: 1\n",
      "Blacklist Regex: N/A\n",
      "Maximum Response Size: 6553500\n",
      "Maximum Request Size: 6553500\n",
      "Limit Maximum Image Pixels: true\n",
      "Prefer direct buffer: false\n",
      "Allowed Urls: [file://.*|http(s)?://.*]\n",
      "Custom python dependency for model allowed: false\n",
      "Enable metrics API: true\n",
      "Metrics mode: log\n",
      "Disable system metrics: true\n",
      "Workflow Store: /.sagemaker/ts/models\n",
      "Model config: N/A\n",
      "2023-11-24T16:24:43,143 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
      "2023-11-24T16:24:43,162 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\n",
      "2023-11-24T16:24:43,166 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "2023-11-24T16:24:43,166 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "2023-11-24T16:24:43,169 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "2023-11-24T16:24:43,179 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "2023-11-24T16:24:43,262 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "2023-11-24T16:24:43,263 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "2023-11-24T16:24:43,264 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "2023-11-24T16:24:43,264 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "2023-11-24T16:24:43.482:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=70, BatchStrategy=MULTI_RECORD\n",
      "2023-11-24T16:24:43,422 [INFO ] pool-2-thread-2 ACCESS_LOG - /169.254.255.130:57092 \"GET /ping HTTP/1.1\" 200 22\n",
      "2023-11-24T16:24:43,427 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:17ed6442914d,timestamp:1700843083\n",
      "2023-11-24T16:24:43,458 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:57108 \"GET /execution-parameters HTTP/1.1\" 404 1\n",
      "2023-11-24T16:24:43,459 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:17ed6442914d,timestamp:1700843083\n",
      "Model server started.\n",
      "2023-11-24T16:24:43,645 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:17ed6442914d,timestamp:1700843083\n",
      "2023-11-24T16:24:43,422 [INFO ] pool-2-thread-2 ACCESS_LOG - /169.254.255.130:57092 \"GET /ping HTTP/1.1\" 200 22\n",
      "2023-11-24T16:24:43,427 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:17ed6442914d,timestamp:1700843083\n",
      "2023-11-24T16:24:43,458 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:57108 \"GET /execution-parameters HTTP/1.1\" 404 1\n",
      "2023-11-24T16:24:43,459 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:17ed6442914d,timestamp:1700843083\n",
      "Model server started.\n",
      "2023-11-24T16:24:43,645 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:17ed6442914d,timestamp:1700843083\n",
      "2023-11-24T16:24:44,780 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=70\n",
      "2023-11-24T16:24:44,782 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "2023-11-24T16:24:44,797 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\n",
      "2023-11-24T16:24:44,798 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]70\n",
      "2023-11-24T16:24:44,798 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "2023-11-24T16:24:44,799 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\n",
      "2023-11-24T16:24:44,780 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=70\n",
      "2023-11-24T16:24:44,782 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "2023-11-24T16:24:44,797 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\n",
      "2023-11-24T16:24:44,798 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]70\n",
      "2023-11-24T16:24:44,798 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "2023-11-24T16:24:44,799 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\n",
      "2023-11-24T16:24:44,802 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "2023-11-24T16:24:44,809 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "2023-11-24T16:24:44,810 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1700843084810\n",
      "2023-11-24T16:24:44,833 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "2023-11-24T16:24:44,802 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "2023-11-24T16:24:44,809 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "2023-11-24T16:24:44,810 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1700843084810\n",
      "2023-11-24T16:24:44,833 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "2023-11-24T16:24:46,446 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Load pretrained SentenceTransformer: all-MiniLM-L12-v2\n",
      "2023-11-24T16:24:46,529 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,530 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - .gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,531 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - .gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 7.70MB/s]\n",
      "2023-11-24T16:24:46,566 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,566 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,566 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - 1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 1.44MB/s]\n",
      "2023-11-24T16:24:46,606 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,607 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,607 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 55.7MB/s]\n",
      "2023-11-24T16:24:46,642 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,643 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - config.json:   0%|          | 0.00/573 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,644 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - config.json: 100%|██████████| 573/573 [00:00<00:00, 4.39MB/s]\n",
      "2023-11-24T16:24:46,680 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,446 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Load pretrained SentenceTransformer: all-MiniLM-L12-v2\n",
      "2023-11-24T16:24:46,529 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,530 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - .gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,531 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - .gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 7.70MB/s]\n",
      "2023-11-24T16:24:46,566 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,566 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,566 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - 1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 1.44MB/s]\n",
      "2023-11-24T16:24:46,606 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,607 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,607 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 55.7MB/s]\n",
      "2023-11-24T16:24:46,642 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,643 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - config.json:   0%|          | 0.00/573 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,644 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - config.json: 100%|██████████| 573/573 [00:00<00:00, 4.39MB/s]\n",
      "2023-11-24T16:24:46,680 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,681 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,681 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 634kB/s]\n",
      "2023-11-24T16:24:46,711 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,712 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,712 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 129MB/s]\n",
      "2023-11-24T16:24:46,758 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,886 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,998 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - pytorch_model.bin:  39%|███▉      | 52.4M/134M [00:00<00:00, 413MB/s]\n",
      "2023-11-24T16:24:47,114 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - pytorch_model.bin:  71%|███████   | 94.4M/134M [00:00<00:00, 391MB/s]\n",
      "2023-11-24T16:24:47,118 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - pytorch_model.bin: 100%|██████████| 134M/134M [00:00<00:00, 367MB/s] \n",
      "2023-11-24T16:24:47,119 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - pytorch_model.bin: 100%|██████████| 134M/134M [00:00<00:00, 371MB/s]\n",
      "2023-11-24T16:24:47,150 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,151 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,151 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 427kB/s]\n",
      "2023-11-24T16:24:47,182 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,182 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,182 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 883kB/s]\n",
      "2023-11-24T16:24:47,215 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,227 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,228 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 37.4MB/s]\n",
      "2023-11-24T16:24:47,260 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,260 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - tokenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,261 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - tokenizer_config.json: 100%|██████████| 352/352 [00:00<00:00, 2.86MB/s]\n",
      "2023-11-24T16:24:47,296 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,296 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,297 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 74.2MB/s]\n",
      "2023-11-24T16:24:47,333 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,337 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,338 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 55.9MB/s]\n",
      "2023-11-24T16:24:47,370 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,371 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,371 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - modules.json: 100%|██████████| 349/349 [00:00<00:00, 2.78MB/s]\n",
      "2023-11-24T16:24:46,681 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,681 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 634kB/s]\n",
      "2023-11-24T16:24:46,711 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,712 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,712 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 129MB/s]\n",
      "2023-11-24T16:24:46,758 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:46,886 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:46,998 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - pytorch_model.bin:  39%|███▉      | 52.4M/134M [00:00<00:00, 413MB/s]\n",
      "2023-11-24T16:24:47,114 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - pytorch_model.bin:  71%|███████   | 94.4M/134M [00:00<00:00, 391MB/s]\n",
      "2023-11-24T16:24:47,118 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - pytorch_model.bin: 100%|██████████| 134M/134M [00:00<00:00, 367MB/s] \n",
      "2023-11-24T16:24:47,119 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - pytorch_model.bin: 100%|██████████| 134M/134M [00:00<00:00, 371MB/s]\n",
      "2023-11-24T16:24:47,150 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,151 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,151 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 427kB/s]\n",
      "2023-11-24T16:24:47,182 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,182 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,182 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 883kB/s]\n",
      "2023-11-24T16:24:47,215 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,227 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,228 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 37.4MB/s]\n",
      "2023-11-24T16:24:47,260 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,260 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - tokenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,261 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - tokenizer_config.json: 100%|██████████| 352/352 [00:00<00:00, 2.86MB/s]\n",
      "2023-11-24T16:24:47,296 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,296 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,297 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 74.2MB/s]\n",
      "2023-11-24T16:24:47,333 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,337 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,338 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 55.9MB/s]\n",
      "2023-11-24T16:24:47,370 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:47,371 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]\n",
      "2023-11-24T16:24:47,371 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - modules.json: 100%|██████████| 349/349 [00:00<00:00, 2.78MB/s]\n",
      "2023-11-24T16:24:48,046 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Use pytorch device: cuda\n",
      "2023-11-24T16:24:48,046 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Use pytorch device: cuda\n",
      "2023-11-24T16:24:49,608 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4776\n",
      "2023-11-24T16:24:49,609 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:6434.0|#WorkerName:W-9000-model_1.0,Level:Host|#hostname:17ed6442914d,timestamp:1700843089\n",
      "2023-11-24T16:24:49,609 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:23.0|#Level:Host|#hostname:17ed6442914d,timestamp:1700843089\n",
      "2023-11-24T16:24:49,610 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1700843089610\n",
      "2023-11-24T16:24:49,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1700843089\n",
      "2023-11-24T16:24:49,608 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4776\n",
      "2023-11-24T16:24:49,609 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:6434.0|#WorkerName:W-9000-model_1.0,Level:Host|#hostname:17ed6442914d,timestamp:1700843089\n",
      "2023-11-24T16:24:49,609 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:23.0|#Level:Host|#hostname:17ed6442914d,timestamp:1700843089\n",
      "2023-11-24T16:24:49,610 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1700843089610\n",
      "2023-11-24T16:24:49,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1700843089\n",
      "2023-11-24T16:24:51,239 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:51,243 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Map:   0%|          | 0/3 [00:00<?, ? examples/s]\n",
      "2023-11-24T16:24:51,243 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:51,239 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:51,243 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Map:   0%|          | 0/3 [00:00<?, ? examples/s]\n",
      "2023-11-24T16:24:51,243 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:52,297 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Batches:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\n",
      "2023-11-24T16:24:52,298 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:52,298 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Batches: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]#033[A\n",
      "2023-11-24T16:24:52,298 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Batches: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "2023-11-24T16:24:52,300 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:52,301 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Map: 100%|██████████| 3/3 [00:01<00:00,  2.83 examples/s]\n",
      "2023-11-24T16:24:52,302 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Map: 100%|██████████| 3/3 [00:01<00:00,  2.82 examples/s]\n",
      "2023-11-24T16:24:52,311 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Predictions on 3 samples\n",
      "2023-11-24T16:24:52,311 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:2699.46|#ModelName:model,Level:Model|#hostname:17ed6442914d,1700843092,6240a67a-0463-480b-9f7d-5f7df07733db, pattern=[METRICS]\n",
      "2023-11-24T16:24:52,312 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.ms:2699.46|#ModelName:model,Level:Model|#hostname:17ed6442914d,requestID:6240a67a-0463-480b-9f7d-5f7df07733db,timestamp:1700843092\n",
      "2023-11-24T16:24:52,312 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:57112 \"POST /invocations HTTP/1.1\" 200 8670\n",
      "2023-11-24T16:24:52,313 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:17ed6442914d,timestamp:1700843092\n",
      "2023-11-24T16:24:52,313 [INFO ] W-9000-model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:8665098.148|#model_name:model,model_version:default|#hostname:17ed6442914d,timestamp:1700843092\n",
      "2023-11-24T16:24:52,314 [INFO ] W-9000-model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:5962455.432|#model_name:model,model_version:default|#hostname:17ed6442914d,timestamp:1700843092\n",
      "2023-11-24T16:24:52,314 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.Milliseconds:5962.0|#Level:Host|#hostname:17ed6442914d,timestamp:1700843092\n",
      "2023-11-24T16:24:52,314 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2702\n",
      "2023-11-24T16:24:52,314 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:17ed6442914d,timestamp:1700843092\n",
      "2023-11-24T16:24:52,297 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Batches:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\n",
      "2023-11-24T16:24:52,298 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:52,298 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Batches: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]#033[A\n",
      "2023-11-24T16:24:52,298 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Batches: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "2023-11-24T16:24:52,300 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - \n",
      "2023-11-24T16:24:52,301 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Map: 100%|██████████| 3/3 [00:01<00:00,  2.83 examples/s]\n",
      "2023-11-24T16:24:52,302 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Map: 100%|██████████| 3/3 [00:01<00:00,  2.82 examples/s]\n",
      "2023-11-24T16:24:52,311 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Predictions on 3 samples\n",
      "2023-11-24T16:24:52,311 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:2699.46|#ModelName:model,Level:Model|#hostname:17ed6442914d,1700843092,6240a67a-0463-480b-9f7d-5f7df07733db, pattern=[METRICS]\n",
      "2023-11-24T16:24:52,312 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.ms:2699.46|#ModelName:model,Level:Model|#hostname:17ed6442914d,requestID:6240a67a-0463-480b-9f7d-5f7df07733db,timestamp:1700843092\n",
      "2023-11-24T16:24:52,312 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:57112 \"POST /invocations HTTP/1.1\" 200 8670\n",
      "2023-11-24T16:24:52,313 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:17ed6442914d,timestamp:1700843092\n",
      "2023-11-24T16:24:52,313 [INFO ] W-9000-model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:8665098.148|#model_name:model,model_version:default|#hostname:17ed6442914d,timestamp:1700843092\n",
      "2023-11-24T16:24:52,314 [INFO ] W-9000-model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:5962455.432|#model_name:model,model_version:default|#hostname:17ed6442914d,timestamp:1700843092\n",
      "2023-11-24T16:24:52,314 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.Milliseconds:5962.0|#Level:Host|#hostname:17ed6442914d,timestamp:1700843092\n",
      "2023-11-24T16:24:52,314 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2702\n",
      "2023-11-24T16:24:52,314 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:17ed6442914d,timestamp:1700843092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transformer.transform(\n",
    "    \"s3://llama-weights-us/input_data/\",\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\",\n",
    "    wait=True,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ישראל חי עם</td>\n",
       "      <td>-0.028912</td>\n",
       "      <td>0.114269</td>\n",
       "      <td>-0.114297</td>\n",
       "      <td>-0.022898</td>\n",
       "      <td>-0.009962</td>\n",
       "      <td>-0.046699</td>\n",
       "      <td>0.148093</td>\n",
       "      <td>-0.026738</td>\n",
       "      <td>0.058141</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011244</td>\n",
       "      <td>0.050298</td>\n",
       "      <td>-0.031433</td>\n",
       "      <td>0.059069</td>\n",
       "      <td>0.039949</td>\n",
       "      <td>0.012332</td>\n",
       "      <td>0.011578</td>\n",
       "      <td>0.028020</td>\n",
       "      <td>-0.042433</td>\n",
       "      <td>-0.061470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>עם ישראל חי</td>\n",
       "      <td>-0.024890</td>\n",
       "      <td>0.123166</td>\n",
       "      <td>-0.112473</td>\n",
       "      <td>-0.032596</td>\n",
       "      <td>-0.002534</td>\n",
       "      <td>-0.045722</td>\n",
       "      <td>0.158048</td>\n",
       "      <td>-0.020617</td>\n",
       "      <td>0.050401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.040327</td>\n",
       "      <td>-0.031516</td>\n",
       "      <td>0.049747</td>\n",
       "      <td>0.031777</td>\n",
       "      <td>0.012196</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.040684</td>\n",
       "      <td>-0.049453</td>\n",
       "      <td>-0.061965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>תנו לצהל לנצח</td>\n",
       "      <td>-0.060326</td>\n",
       "      <td>0.103238</td>\n",
       "      <td>-0.001080</td>\n",
       "      <td>0.061596</td>\n",
       "      <td>0.012187</td>\n",
       "      <td>-0.081065</td>\n",
       "      <td>0.112682</td>\n",
       "      <td>-0.028075</td>\n",
       "      <td>0.074386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014554</td>\n",
       "      <td>-0.019529</td>\n",
       "      <td>0.043713</td>\n",
       "      <td>0.017257</td>\n",
       "      <td>-0.066258</td>\n",
       "      <td>0.008436</td>\n",
       "      <td>-0.038106</td>\n",
       "      <td>-0.008491</td>\n",
       "      <td>-0.036781</td>\n",
       "      <td>-0.050527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0    ישראל חי עם -0.028912  0.114269 -0.114297 -0.022898 -0.009962 -0.046699   \n",
       "1    עם ישראל חי -0.024890  0.123166 -0.112473 -0.032596 -0.002534 -0.045722   \n",
       "2  תנו לצהל לנצח -0.060326  0.103238 -0.001080  0.061596  0.012187 -0.081065   \n",
       "\n",
       "        7         8         9    ...       375       376       377       378  \\\n",
       "0  0.148093 -0.026738  0.058141  ... -0.011244  0.050298 -0.031433  0.059069   \n",
       "1  0.158048 -0.020617  0.050401  ...  0.001951  0.040327 -0.031516  0.049747   \n",
       "2  0.112682 -0.028075  0.074386  ...  0.014554 -0.019529  0.043713  0.017257   \n",
       "\n",
       "        379       380       381       382       383       384  \n",
       "0  0.039949  0.012332  0.011578  0.028020 -0.042433 -0.061470  \n",
       "1  0.031777  0.012196  0.004450  0.040684 -0.049453 -0.061965  \n",
       "2 -0.066258  0.008436 -0.038106 -0.008491 -0.036781 -0.050527  \n",
       "\n",
       "[3 rows x 385 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('s3://llama-weights-us/input_data/texts.csv.out', header=None)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
